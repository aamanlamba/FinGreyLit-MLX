{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune meta-llama/Llama-3.2-1B-Instruct using mlx framework and llm-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.2-1B-Instruct\n",
      "Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# set model name etc.\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "MODEL_SHORT_NAME = MODEL_NAME.split('/')[-1]\n",
    "SUFFIX = \"FinGreyLit-MLX\"\n",
    "#SLICE = 1\n",
    "print(MODEL_NAME)\n",
    "print(MODEL_SHORT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 64 train records\n",
      "Wrote 15 test records\n",
      "Wrote 6 eval records\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare fine-tuning dataset\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(42)  # for deterministic sampling of test set\n",
    "\n",
    "train_files = glob.glob(\"../../llm-dataset/*-train.jsonl\") + glob.glob(\"../../llm-dataset/norrep.jsonl\")\n",
    "test_files = glob.glob(\"../../llm-dataset/*-test.jsonl\")\n",
    "\n",
    "EVAL_SIZE = 6  # how many documents to evaluate (i.e. calculate loss) on during fine-tuning\n",
    "SYSTEM_PROMPT = \"You are a skilled librarian specialized in meticulous cataloguing of digital documents.\"\n",
    "INSTRUCTION = \"Extract metadata from this document. Return as JSON.\"\n",
    "\n",
    "def preprocess_sample(sample):\n",
    "    output = json.dumps(sample[\"ground_truth\"])\n",
    "    input_ = json.dumps(sample[\"content\"])\n",
    "    # ShareGPT format\n",
    "    conversations = [\n",
    "        {'from': 'system', 'value': SYSTEM_PROMPT},\n",
    "        {'from': 'user', 'value': INSTRUCTION + \"\\n\\n\" + input_},\n",
    "        {'from': 'gpt', 'value': output}\n",
    "    ]\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def dataset_to_records(files):\n",
    "    records = []\n",
    "    for filename in files:\n",
    "        with open(filename) as infile:\n",
    "            for line in infile:\n",
    "                sample = json.loads(line)\n",
    "                records.append(preprocess_sample(sample))\n",
    "    return records\n",
    "\n",
    "def write_jsonl(records, filename):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        for record in records:\n",
    "            json.dump(record, outfile)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "train_recs = dataset_to_records(train_files)\n",
    "random.shuffle(train_recs)\n",
    "write_jsonl(train_recs, \"./data/mlx-train.jsonl\")\n",
    "print(f\"Wrote {len(train_recs)} train records\")\n",
    "\n",
    "test_recs = dataset_to_records(test_files)\n",
    "write_jsonl(test_recs, \"./data/mlx-test.jsonl\")\n",
    "print(f\"Wrote {len(test_recs)} test records\")\n",
    "\n",
    "eval_recs = random.sample(test_recs, EVAL_SIZE)\n",
    "write_jsonl(eval_recs, \"./data/mlx-eval.jsonl\")\n",
    "print(f\"Wrote {len(eval_recs)} eval records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aamanlamba/Documents/Code/FinGreyLit/experiments/mlx-finetune-llm/data\n",
      "Conversion completed!\n"
     ]
    }
   ],
   "source": [
    "!python convert-datasets-to-llama.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aamanlamba/Documents/Code/FinGreyLit/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load and finetune LLM\n",
    "from mlx_lm import generate,load\n",
    "from huggingface_hub import login\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the client with your API key\n",
    "hf_accesstoken = os.environ.get(\"mlx_finetuning_api_key\")\n",
    "login(hf_accesstoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model, tokenizer = load(\"meta-llama/Llama-3.2-1B-Instruct\") \n",
    "#load(\"google/gemma-2-2b-it\")\n",
    "#generate prompt and response\n",
    "prompt = \"You are a skilled librarian \\\n",
    "        specialized in meticulous cataloguing of digital documents.\\\n",
    "        Extract metadata from this document. Return as JSON.\\\n",
    "        \\{\\\"pdfinfo\\\": {\\\"author\\\": \\\"Johanna Elisabet Glader\\\", \\\n",
    "        \\\"creationDate\\\": \\\"D:20200226153952+02'00'\\\", \\\"modDate\\\": \\\n",
    "        \\\"D:20200226154128+02'00'\\\"}, \\\"pages\\\": [{\\\"page\\\": 1, \\\"text\\\": \\\n",
    "        \\\"This is an electronic reprint of the original article. \\\n",
    "        This reprint may differ from the original in pagination and typographic \\\n",
    "        detail.\\\\nPlease cite the original version:\\\\nCynthia S\\\\u00f6derbacka (2019). \\\n",
    "            Energy storage demo environment in Technobothnia. Vaasa\\\\nInsider, 11.12.2019.\\\"}, \\\n",
    "                {\\\"page\\\": 2, \\\"text\\\": \\\"Energy Storage Demo Environment in\\\\nTechnobothnia\\\\nWhen \\\n",
    "                discourses about the benefits that come with renewable energy sources come up, \\\n",
    "                    one cannot avoid the discussion going into the irregular tendencies of these \\\n",
    "                        sources especially with reference to solar and wind energy and the challenges \\\n",
    "                            that come with that.\\\\nUnfortunately, the future does not have much room for \\\n",
    "                                fossil fuels, the era of renewable energy sources is here and will continue to grow. \\\n",
    "                                    According to the International Renewable\\\\nNovia University of Applied Sciences \\\n",
    "                                        (Novia UAS), \\\\u00c5bo Akademi University (\\\\u00c5A), and\\\\nVaasa University \\\n",
    "                                            of Applied Sciences (VAMK) have partnered up under \\\n",
    "                                                the \\\\u201cEnergy\\\\nEducation & Research Laboratory.\\\"},\\\n",
    "                                                      {\\\"page\\\": 4, \\\"text\\\": \\\"A thesis worker from \\\\u00c5A is \\\n",
    "                                                      focusing on delivering a Phase Changing Materials \\\n",
    "                                                        (PCM)\\\"}, {\\\"page\\\": 5, \\\"text\\\": \\\"The Lab Engineer at \\\n",
    "                                                            Novia UAS is in charge of this.\\\\nAuthor: \\\n",
    "                                                                Cynthia S\\\\u00f6derbacka- Project Manager \\\n",
    "                                                                    & Lecturer at Novia UAS\\\"}]}\\\"}, \\\n",
    "                                                                        {\\\"from\\\": \\\"gpt\\\", \\\"value\\\": \\\"{\\\"language\\\": \\\n",
    "                                                                        \\\"en\\\", \\\"title\\\": \\\"Energy storage demo environment in \\\n",
    "                                                                        Technobothnia\\\", \\\"creator\\\": [\\\"S\\\\u00f6derbacka, Cynthia\\\"],\\\n",
    "                                                                              \\\"year\\\": \\\"2019\\\", \\\"publisher\\\": [\\\"Vaasa Insider\\\"],\\\n",
    "                                                                                \\\"type_coar\\\": \\\"newspaper article\\\"}\\\"}]}    \"\n",
    "print(prompt)\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True, max_tokens = 100)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"json\", \n",
    "                  data_files={\"train\": \"./data/output_mlx-train.jsonl\", \n",
    "                              \"test\": \"./data/output_mlx-test.jsonl\", \n",
    "                              \"eval\": \"./data/output_mlx-eval.jsonl\"})\n",
    "\n",
    "# convert dataset to dataframe\n",
    "import pandas as pd\n",
    "df_train = pd.DataFrame(ds['train'])\n",
    "df_test = pd.DataFrame(ds['test'])\n",
    "df_eval = pd.DataFrame(ds['eval'])\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n",
    "print(df_eval.head())\n",
    "# convert dataframe to list for mlx\n",
    "#convert dataset to list for mlx\n",
    "train_set = df_train['text'].tolist()\n",
    "dev_set = df_eval['text'].tolist()\n",
    "test_set = df_test['text'].tolist()\n",
    "\n",
    "def preprocess(dataset):\n",
    "    return dataset[\"text\"].tolist()\n",
    "#train_list = df_train.to_dict(orient='records')\n",
    "#test_list = df_test.to_dict(orient='records')\n",
    "#eval_list = df_eval.to_dict(orient='records')\n",
    "#train_set, dev_set, test_set = map(preprocess, (df_train,df_eval,df_test))\n",
    "\n",
    "print(train_set[0])\n",
    "print(dev_set[0])\n",
    "print(test_set[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model finetuning setup\n",
    "import matplotlib.pyplot as plt\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm import load, generate\n",
    "from mlx_lm.tuner import train, TrainingArgs \n",
    "from mlx_lm.tuner import linear_to_lora_layers\n",
    "from pathlib import Path\n",
    "import json, time\n",
    "adapter_path = Path(\"./adapters\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "#set LORA parameters\n",
    "lora_config = {\n",
    " \"lora_layers\": 8,\n",
    " \"num_layers\": 8,\n",
    " \"lora_parameters\": {\n",
    "    \"rank\": 8,\n",
    "    \"scale\": 20.0,\n",
    "    \"dropout\": 0.0,\n",
    "}}\n",
    "with open(adapter_path / \"adapter_config.json\", \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)    \n",
    "# Set training parameters\n",
    "training_args = TrainingArgs(\n",
    "    adapter_file=adapter_path / \"adapters.safetensors\",\n",
    "    iters=200,\n",
    "    steps_per_eval=50\n",
    ")\n",
    "#Freeze base model\n",
    "model.freeze()\n",
    "\n",
    "linear_to_lora_layers(model, lora_config[\"lora_layers\"], lora_config[\"lora_parameters\"])\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training model and optimizer - Adam\n",
    "model.train()\n",
    "opt = optim.Adam(learning_rate=1e-5)\n",
    "#create metrics class to measure fine-tuning progress\n",
    "class Metrics:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    def on_train_loss_report(self, info):\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "    def on_val_loss_report(self, info):\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "        \n",
    "metrics = Metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start fine-tuning\n",
    "\n",
    "# Start fine-tuning\n",
    "start_time = time.time()\n",
    "\n",
    "train(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_args,\n",
    "    optimizer = opt,\n",
    "    train_dataset = train_set,\n",
    "    val_dataset = dev_set,\n",
    "    training_callback = metrics\n",
    ")\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Completed finetuning Training in {duration/60:.2f} minutes\")\n",
    "\n",
    "# plot graph of fine-tuning\n",
    "train_its, train_losses = zip(*metrics.train_losses)\n",
    "val_its, val_losses = zip(*metrics.val_losses)\n",
    "plt.plot(train_its, train_losses, '-o')\n",
    "plt.plot(val_its, val_losses, '-o')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Train', \"Valid\"])\n",
    "plt.show() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
