{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28db6e3d",
   "metadata": {},
   "source": [
    "# Import records from Google Sheets\n",
    "\n",
    "The curation of metadata records is done in a [Google Sheets document](https://docs.google.com/spreadsheets/d/1acNnSzn8XrCDFrxf4joqSAUsAQoOgHEvBU-FIt9p9kU/edit?usp=sharing) with five sheets (tabs), one per document type.\n",
    "\n",
    "This notebook will read the current contents of the sheet into a Pandas dataframe. The records are converted into Python dictionaries and then saved into JSONL files, split into separate train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cacaff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article     Repository                         Collection  \\\n",
      "0        Doria         ÅA / Kirjoja, sarjoja, ym.   \n",
      "1        Doria         ÅA / Kirjoja, sarjoja, ym.   \n",
      "2        Doria         ÅA / Kirjoja, sarjoja, ym.   \n",
      "3         Taju                         Artikkelit   \n",
      "4      Theseus                 Laurea / Julkaisut   \n",
      "..         ...                                ...   \n",
      "184   OuluRepo  Oulun yliopisto / Avoin saatavuus   \n",
      "185   OuluRepo  Oulun yliopisto / Avoin saatavuus   \n",
      "186   OuluRepo  Oulun yliopisto / Avoin saatavuus   \n",
      "187   OuluRepo  Oulun yliopisto / Avoin saatavuus   \n",
      "188   OuluRepo  Oulun yliopisto / Avoin saatavuus   \n",
      "\n",
      "                                                   url       rowid  \\\n",
      "0    https://www.doria.fi/bitstream/handle/10024/18...    article2   \n",
      "1    https://www.doria.fi/bitstream/handle/10024/18...    article3   \n",
      "2    https://www.doria.fi/bitstream/handle/10024/17...    article4   \n",
      "3    https://taju.uniarts.fi/bitstream/handle/10024...    article5   \n",
      "4    https://www.theseus.fi/bitstream/handle/10024/...    article6   \n",
      "..                                                 ...         ...   \n",
      "184  https://oulurepo.oulu.fi/bitstream/handle/1002...  article186   \n",
      "185  https://oulurepo.oulu.fi/bitstream/handle/1002...  article187   \n",
      "186  https://oulurepo.oulu.fi/bitstream/handle/1002...  article188   \n",
      "187  https://oulurepo.oulu.fi/bitstream/handle/1002...  article189   \n",
      "188  https://oulurepo.oulu.fi/bitstream/handle/1002...  article190   \n",
      "\n",
      "                                                Status language  \\\n",
      "0                                                    S       en   \n",
      "1                                                    S       en   \n",
      "2                                                    S       sv   \n",
      "3                                   V (sarjajulkaisu?)       en   \n",
      "4    V (arkistolisäsivuja ei laskettu; Juuso Ala-Ky...       fi   \n",
      "..                                                 ...      ...   \n",
      "184                                                          se   \n",
      "185                                                          se   \n",
      "186                                                          se   \n",
      "187                                                          se   \n",
      "188                                                          se   \n",
      "\n",
      "                                                 title  \\\n",
      "0    ‘One of the most important questions that huma...   \n",
      "1       ‘The wrath of God on children of disobedience’   \n",
      "2    ”Den som pekar på andras brister visar därigen...   \n",
      "3    \"Great horizons flooded with the alien light o...   \n",
      "4    \"Nenästä kiinni ja ponnistus ja hyppy!\" : kirj...   \n",
      "..                                                 ...   \n",
      "184    dat, dát, diet, duot, dot ja de do dot do doppe   \n",
      "185           ツンドラ, تندرا ja eará Sámis gárgidan sánit   \n",
      "186  Davvisámegiela máŋggaidlogu illatiivva variašu...   \n",
      "187  Čalbmi čalmmis ja suoldnečalmmit suoidnečalmmi...   \n",
      "188  Tē lijen sōmes pālen ai aktan saijesne sāmi’ :...   \n",
      "\n",
      "                                             alt_title  \\\n",
      "0    Salafism as Islamic deferentialist fundamental...   \n",
      "1    COVID-19 in the theology and ideology of the W...   \n",
      "2                                                        \n",
      "3                                                        \n",
      "4                                                        \n",
      "..                                                 ...   \n",
      "184                                                      \n",
      "185                                                      \n",
      "186                                                      \n",
      "187                                                      \n",
      "188                                                      \n",
      "\n",
      "                                               creator editor  ...  \\\n",
      "0                     Olsson, Susanne\\nSvensson, Jonas         ...   \n",
      "1                                        Östling, Erik         ...   \n",
      "2                                      Rudberg, Pontus         ...   \n",
      "3                                      Järvinen, Hanna         ...   \n",
      "4    Virrankoski, Antti\\nManninen, Soile\\nSuikka, M...         ...   \n",
      "..                                                 ...    ...  ...   \n",
      "184                                    Ylikoski, Jussi         ...   \n",
      "185                                    Ylikoski, Jussi         ...   \n",
      "186                                     Rasmus, Sierge         ...   \n",
      "187                                    Ylikoski, Jussi         ...   \n",
      "188                                    Ylikoski, Jussi         ...   \n",
      "\n",
      "    relation/numberinseries relation/volume series/name X  \\\n",
      "0                                        12                 \n",
      "1                                        11                 \n",
      "2                                        31                 \n",
      "3                                                           \n",
      "4                         2              52                 \n",
      "..                      ...             ...           ...   \n",
      "184                                                         \n",
      "185                                                         \n",
      "186                                                         \n",
      "187                                                         \n",
      "188                                                         \n",
      "\n",
      "    series/sortingnumber X series/volume X series/year subject/degreeprogram  \\\n",
      "0                                                                              \n",
      "1                                                                              \n",
      "2                                                                              \n",
      "3                                                                              \n",
      "4                                                 2020                         \n",
      "..                     ...             ...         ...                   ...   \n",
      "184                                                                            \n",
      "185                                                                            \n",
      "186                                                                            \n",
      "187                                                                            \n",
      "188                                                                            \n",
      "\n",
      "    subject/discipline                                           type/okm  \\\n",
      "0                       A1 Alkuperäisartikkeli tieteellisessä aikakaus...   \n",
      "1                       A1 Alkuperäisartikkeli tieteellisessä aikakaus...   \n",
      "2                                            D1 Artikkeli ammattilehdessä   \n",
      "3                       A1 Alkuperäisartikkeli tieteellisessä aikakaus...   \n",
      "4                                            D1 Artikkeli ammattilehdessä   \n",
      "..                 ...                                                ...   \n",
      "184                                                                         \n",
      "185                                                                         \n",
      "186                                                                         \n",
      "187                                                                         \n",
      "188                                                                         \n",
      "\n",
      "    type/ontasot  \n",
      "0                 \n",
      "1                 \n",
      "2                 \n",
      "3                 \n",
      "4                 \n",
      "..           ...  \n",
      "184               \n",
      "185               \n",
      "186               \n",
      "187               \n",
      "188               \n",
      "\n",
      "[189 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the metadata from the Google Sheets document into Pandas dataframes\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "\n",
    "DOC_ID = \"1acNnSzn8XrCDFrxf4joqSAUsAQoOgHEvBU-FIt9p9kU\"  # Google Sheets id\n",
    "LANGUAGES = ('fi', 'sv', 'en', 'se')\n",
    "#go through only articles for now\n",
    "SHEET_NAMES = {\n",
    " #   \"thes\": \"Theses\",\n",
    "  #  \"docthes\": \"Doctoral theses\",\n",
    "   # \"report\": \"Reports\",\n",
    "    #\"book\": \"Books\",\n",
    "    \"article\": \"Articles\"\n",
    "}\n",
    "\n",
    "def read_sheet(doc_id, sheet_name, sheet_id):\n",
    "    csv_url = f\"https://docs.google.com/spreadsheets/d/{doc_id}/gviz/tq?tqx=out:csv&sheet={urllib.parse.quote(sheet_name)}\"\n",
    "    df = pd.read_csv(csv_url, dtype=str, na_filter=False)\n",
    "    # add a new \"rowid\" column, with values like \"thes37\", in between other housekeeping columns\n",
    "    df[\"rowid\"] = df.index + 2\n",
    "    df[\"rowid\"] = df[\"rowid\"].apply(lambda x: f\"{sheet_id}{x}\")\n",
    "    df.insert(3, \"rowid\", df.pop(\"rowid\"))\n",
    "    # restrict to monolingual records in the languages we are interested in\n",
    "    df = df.loc[df[\"language\"].isin(LANGUAGES)]\n",
    "    df.head(2)\n",
    "    return df\n",
    "\n",
    "sheets = {sheet_id: read_sheet(DOC_ID, sheet_name, sheet_id)\n",
    "          for sheet_id, sheet_name in SHEET_NAMES.items()}\n",
    "for i, (sheet_id, sheet_data) in enumerate(sheets.items()): \n",
    "    if i < 3: \n",
    "        print(sheet_id, sheet_data) \n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94bf161c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>TOTAL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fi</th>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sv</th>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>se</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL</th>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          article  TOTAL\n",
       "language                \n",
       "en             79     79\n",
       "fi             70     70\n",
       "sv             33     33\n",
       "se              7      7\n",
       "TOTAL         189    189"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate language statistics for each sheet (document type) and combine into an overview\n",
    "\n",
    "language_counts = {sheet_id: sheet[\"language\"].value_counts().rename(sheet_id).astype(int)\n",
    "                   for sheet_id, sheet in sheets.items()}\n",
    "\n",
    "langstat = pd.concat(language_counts.values(), axis=1).fillna(0).astype(int)\n",
    "langstat['TOTAL'] = langstat.sum(axis=1)\n",
    "langstat.loc['TOTAL'] = langstat.sum()\n",
    "langstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf91825e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>TOTAL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repository</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Theseus</th>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taju</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doria</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Osuva</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kaisu</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OuluRepo</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Julkari</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL</th>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            article  TOTAL\n",
       "Repository                \n",
       "Theseus         113    113\n",
       "Taju             20     20\n",
       "Doria            16     16\n",
       "Osuva            16     16\n",
       "Kaisu            13     13\n",
       "OuluRepo          7      7\n",
       "Julkari           4      4\n",
       "TOTAL           189    189"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate by-repository statistics for each sheet (document type) and combine into an overview\n",
    "\n",
    "repo_counts = {sheet_id: sheet[\"Repository\"].value_counts().rename(sheet_id).astype(int)\n",
    "               for sheet_id, sheet in sheets.items()}\n",
    "\n",
    "repostat = pd.concat(repo_counts.values(), axis=1).fillna(0).astype(int)\n",
    "repostat['TOTAL'] = repostat.sum(axis=1)\n",
    "repostat.loc['TOTAL'] = repostat.sum()\n",
    "repostat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e056b647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>TOTAL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_coar</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>journal article</th>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>research article</th>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newspaper article</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conference paper</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blog post</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book review</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>editorial</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review article</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL</th>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   article  TOTAL\n",
       "type_coar                        \n",
       "journal article         85     85\n",
       "research article        45     45\n",
       "newspaper article       21     21\n",
       "conference paper        17     17\n",
       "blog post               13     13\n",
       "book review              4      4\n",
       "editorial                3      3\n",
       "review article           1      1\n",
       "TOTAL                  189    189"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate by-COAR-type statistics for each sheet (document type) and combine into an overview\n",
    "\n",
    "coar_counts = {sheet_id: sheet[\"type_coar\"].value_counts().rename(sheet_id).astype(int)\n",
    "               for sheet_id, sheet in sheets.items()}\n",
    "\n",
    "coarstat = pd.concat(coar_counts.values(), axis=1).fillna(0).astype(int)\n",
    "coarstat['TOTAL'] = coarstat.sum(axis=1)\n",
    "coarstat.loc['TOTAL'] = coarstat.sum()\n",
    "coarstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926d8ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article-fi:\twrote 70 records (57 train, 13 test (18.6 %))\n",
      "article-sv:\twrote 33 records (28 train, 5 test (15.2 %))\n",
      "article-en:\twrote 79 records (64 train, 15 test (19.0 %))\n",
      "article-se:\twrote 7 records (6 train, 1 test (14.3 %))\n"
     ]
    }
   ],
   "source": [
    "# every 1 out of TRAIN_TEST_SPLIT_FACTOR records (on average) will be placed in test set\n",
    "TRAIN_TEST_SPLIT_FACTOR = 4\n",
    "\n",
    "import json\n",
    "import re\n",
    "import zlib\n",
    "\n",
    "def url_to_id(url):\n",
    "    return re.sub(r\"(.+)/bitstream/handle/(\\d+)/(\\d+)/.*\", r\"\\1/handle/\\2/\\3\", url)\n",
    "\n",
    "def convert_colname(col):\n",
    "    if col in ('rowid', 'url'):\n",
    "        return col\n",
    "    elif col == 'Repository':\n",
    "        return 'repository'\n",
    "    return 'dc.' + '.'.join(col.split('/'))\n",
    "\n",
    "def filter_vals(vals):\n",
    "    # remove values marked with [], meaning that they can't be directly inferred from the document\n",
    "    # this works on lists of values as well as individual (string) values\n",
    "    if isinstance(vals, list):\n",
    "        return [v for v in vals if v and not (v.startswith('[') and v.endswith(']'))]\n",
    "\n",
    "    val = vals\n",
    "    if not (val.startswith('[') and val.endswith(']')):\n",
    "        return val\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def row_to_dict(row):\n",
    "    gt = {}\n",
    "\n",
    "    # language\n",
    "    gt[\"language\"] = row[\"language\"].strip()\n",
    "\n",
    "    # title\n",
    "    if (title := filter_vals(row[\"title\"].strip())):\n",
    "        gt[\"title\"] = title\n",
    "\n",
    "    # alt_title\n",
    "    if (alt_titles := filter_vals(row[\"alt_title\"].strip().split(\"\\n\"))):\n",
    "        gt[\"alt_title\"] = alt_titles\n",
    "\n",
    "    # creator\n",
    "    if (creators := filter_vals(row[\"creator\"].strip().split(\"\\n\"))):\n",
    "        gt[\"creator\"] = creators\n",
    "\n",
    "    # year\n",
    "    if (issued := filter_vals(row[\"year\"].strip())):\n",
    "        gt[\"year\"] = issued[:4]  # only include year part\n",
    "\n",
    "    # publisher\n",
    "    if (publishers := filter_vals(row[\"publisher\"].strip().split(\"\\n\"))):\n",
    "        gt[\"publisher\"] = publishers\n",
    "\n",
    "    # DOI\n",
    "    if (doi := filter_vals(row[\"doi\"].strip())):\n",
    "        gt[\"doi\"] = doi\n",
    "        \n",
    "    # e-ISBN\n",
    "    if (eisbns := filter_vals(row[\"e-isbn\"].strip().split(\"\\n\"))):\n",
    "        gt[\"e-isbn\"] = [isbn.replace('-', '') for isbn in eisbns]  # strip dashes in ISBNs\n",
    "\n",
    "    # p-ISBN\n",
    "    if (pisbns := filter_vals(row[\"p-isbn\"].strip().split(\"\\n\"))):\n",
    "        gt[\"p-isbn\"] = [isbn.replace('-', '') for isbn in pisbns]  # strip dashes in ISBNs\n",
    "\n",
    "    # e-ISSN\n",
    "    if (eissn := filter_vals(row[\"e-issn\"].strip())):\n",
    "        gt[\"e-issn\"] = eissn\n",
    "\n",
    "    # p-ISSN\n",
    "    if (pissn := filter_vals(row[\"p-issn\"].strip())):\n",
    "        gt[\"p-issn\"] = pissn\n",
    "\n",
    "    # COAR resource type\n",
    "    if (coartype := filter_vals(row[\"type_coar\"].strip())):\n",
    "        gt[\"type_coar\"] = coartype\n",
    "    \n",
    "    return {\"repository\": row[\"Repository\"].strip(),\n",
    "            \"url\": row[\"url\"].strip(),\n",
    "            \"id\": url_to_id(row[\"url\"].strip()),\n",
    "            \"rowid\": row[\"rowid\"].strip(),\n",
    "            \"ground_truth\": gt}\n",
    "\n",
    "def is_test_record(rec):\n",
    "    \"\"\"deterministically select, based on the record ID, whether a record goes into the train or test set\"\"\"\n",
    "    return 3 * zlib.crc32(rec['id'].encode('utf-8')) % TRAIN_TEST_SPLIT_FACTOR == 1\n",
    "\n",
    "for sheet_id, sheet in sheets.items():\n",
    "    for lang in LANGUAGES:\n",
    "        df = sheet[sheet['language'] == lang]\n",
    "        records = [row_to_dict(row) for _, row in df.iterrows()]\n",
    "        with (open(f\"../metadata/{sheet_id}-{lang}-train.jsonl\", \"w\") as trainfile,\n",
    "              open(f\"../metadata/{sheet_id}-{lang}-test.jsonl\", \"w\") as testfile):\n",
    "            ntrain = ntest = 0\n",
    "            for rec in records:\n",
    "                if is_test_record(rec):\n",
    "                    outfile = testfile\n",
    "                    subset = 'test'\n",
    "                    ntest += 1\n",
    "                else:\n",
    "                    outfile = trainfile\n",
    "                    subset = 'train'\n",
    "                    ntrain += 1\n",
    "                header = {'doctype': sheet_id, 'subset': subset}\n",
    "                json.dump(header | rec, outfile)\n",
    "                outfile.write(\"\\n\")\n",
    "        if records:\n",
    "            print(f\"{sheet_id}-{lang}:\\twrote {len(records)} records ({ntrain} train, {ntest} test ({100*ntest/(ntrain+ntest):.1f} %))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef858a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Statistics about metadata\n",
      "\n",
      "Automatically generated 2025-01-02 17:32:41\n",
      "\n",
      "Type is either:\n",
      " * S: single-value\n",
      " * M: multi-value, number of values given as (mean/max)\n",
      "\n",
      "Percentages represent the coverage of a field in a subset. 100% coverage means the field is always present.\n",
      "\n",
      "## Document counts by language and document type\n",
      "\n",
      "| language   |   article |   TOTAL |\n",
      "|------------|-----------|---------|\n",
      "| en         |        79 |      79 |\n",
      "| fi         |        70 |      70 |\n",
      "| sv         |        33 |      33 |\n",
      "| se         |         7 |       7 |\n",
      "| TOTAL      |       189 |     189 |\n",
      "\n",
      "## Document counts by repository and document type\n",
      "\n",
      "| Repository   |   article |   TOTAL |\n",
      "|--------------|-----------|---------|\n",
      "| Theseus      |       113 |     113 |\n",
      "| Taju         |        20 |      20 |\n",
      "| Doria        |        16 |      16 |\n",
      "| Osuva        |        16 |      16 |\n",
      "| Kaisu        |        13 |      13 |\n",
      "| OuluRepo     |         7 |       7 |\n",
      "| Julkari      |         4 |       4 |\n",
      "| TOTAL        |       189 |     189 |\n",
      "\n",
      "## Document counts by COAR resource type and document type\n",
      "\n",
      "| type_coar         |   article |   TOTAL |\n",
      "|-------------------|-----------|---------|\n",
      "| journal article   |        85 |      85 |\n",
      "| research article  |        45 |      45 |\n",
      "| newspaper article |        21 |      21 |\n",
      "| conference paper  |        17 |      17 |\n",
      "| blog post         |        13 |      13 |\n",
      "| book review       |         4 |       4 |\n",
      "| editorial         |         3 |       3 |\n",
      "| review article    |         1 |       1 |\n",
      "| TOTAL             |       189 |     189 |\n",
      "\n",
      "## Metadata coverage by document type\n",
      "\n",
      "| Field      | Type       | article   |\n",
      "|------------|------------|-----------|\n",
      "| doctype    | S          | 100%      |\n",
      "| subset     | S          | 100%      |\n",
      "| repository | S          | 100%      |\n",
      "| url        | S          | 100%      |\n",
      "| id         | S          | 100%      |\n",
      "| rowid      | S          | 100%      |\n",
      "| language   | S          | 100%      |\n",
      "| title      | S          | 100%      |\n",
      "| alt_title  | M (1.1/2)  | 3%        |\n",
      "| creator    | M (1.9/58) | 96%       |\n",
      "| year       | S          | 95%       |\n",
      "| doi        | S          | 32%       |\n",
      "| type_coar  | S          | 100%      |\n",
      "| publisher  | M (1.0/6)  | 57%       |\n",
      "| e-isbn     | M (1.0/2)  | 3%        |\n",
      "| e-issn     | S          | 12%       |\n",
      "| p-issn     | S          | 3%        |\n",
      "| p-isbn     | M (1.0/2)  | 1%        |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate statistics about the use of metadata fields\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import datetime\n",
    "import glob\n",
    "from statistics import mean\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "langstat_table = tabulate(langstat, headers='keys', tablefmt='github')\n",
    "repostat_table = tabulate(repostat, headers='keys', tablefmt='github')\n",
    "coarstat_table = tabulate(coarstat, headers='keys', tablefmt='github')\n",
    "\n",
    "metadata_files = glob.glob(\"../metadata/*.jsonl\")\n",
    "\n",
    "field_counts = defaultdict(Counter)  # key1: doctype, key2: field\n",
    "doc_counts = Counter()\n",
    "field_types = {}  # key: field, val: S (single) or M (multiple)\n",
    "field_nvals = defaultdict(list)  # key: field, val: list of number of values\n",
    "\n",
    "for mdfile in sorted(metadata_files):\n",
    "    with open(mdfile) as infile:\n",
    "        for line in infile:\n",
    "            rec = json.loads(line)\n",
    "            # flatten the record: include the ground_truth fields at the top level\n",
    "            combined_rec = rec | rec[\"ground_truth\"]\n",
    "            del combined_rec[\"ground_truth\"]\n",
    "            \n",
    "            for fld in combined_rec:\n",
    "                field_counts[rec['doctype']][fld] += 1\n",
    "                field_types[fld] = 'M' if isinstance(combined_rec[fld], list) else 'S'\n",
    "                if field_types[fld] == 'M':\n",
    "                    field_nvals[fld].append(len(combined_rec[fld]))\n",
    "            doc_counts[rec['doctype']] += 1\n",
    "\n",
    "data = []\n",
    "\n",
    "def format_value(val):\n",
    "    if not val:\n",
    "        return '-'\n",
    "    return \"{:.0%}\".format(val)\n",
    "\n",
    "for fld in list(field_types.keys()):\n",
    "\n",
    "    if field_types[fld] == 'M':\n",
    "        mean_val = mean(field_nvals[fld] or [0])\n",
    "        max_val = max(field_nvals[fld] or [0])\n",
    "        ftype = f\"M ({mean_val:.1f}/{max_val})\"\n",
    "    else:\n",
    "        ftype = 'S'\n",
    "\n",
    "\n",
    "    row = {\n",
    "        'Field': fld,\n",
    "        'Type': ftype\n",
    "    }\n",
    "    for doctype in SHEET_NAMES.keys():\n",
    "        row[doctype] = format_value(field_counts[doctype][fld] / doc_counts[doctype])\n",
    "    data.append(row)\n",
    "\n",
    "# Create a Pandas DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert DataFrame to markdown table\n",
    "field_table = tabulate(df, headers='keys', tablefmt='github', showindex=False)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "statfile = f\"\"\"# Statistics about metadata\n",
    "\n",
    "Automatically generated {timestamp}\n",
    "\n",
    "Type is either:\n",
    " * S: single-value\n",
    " * M: multi-value, number of values given as (mean/max)\n",
    "\n",
    "Percentages represent the coverage of a field in a subset. 100% coverage means the field is always present.\n",
    "\n",
    "## Document counts by language and document type\n",
    "\n",
    "{langstat_table}\n",
    "\n",
    "## Document counts by repository and document type\n",
    "\n",
    "{repostat_table}\n",
    "\n",
    "## Document counts by COAR resource type and document type\n",
    "\n",
    "{coarstat_table}\n",
    "\n",
    "## Metadata coverage by document type\n",
    "\n",
    "{field_table}\n",
    "\"\"\"\n",
    "\n",
    "print(statfile)\n",
    "\n",
    "with open('../statistics.md', 'w') as outf:\n",
    "    print(statfile, file=outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9d6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
